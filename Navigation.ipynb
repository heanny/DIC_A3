{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "from dqn_agent import Agent\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PATH = \"environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\74045\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\74045\\anaconda3\\lib\\site-packages (from torchvision) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\74045\\anaconda3\\lib\\site-packages (from torchvision) (1.21.2)\n",
      "Requirement already satisfied: requests in c:\\users\\74045\\anaconda3\\lib\\site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: torch==1.11.0 in c:\\users\\74045\\anaconda3\\lib\\site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\74045\\anaconda3\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\74045\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\74045\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\74045\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\74045\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialise customised Banana Collecter environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_env(ENV_PATH):\n",
    "    # env = UnityEnvironment(file_name=ENV_PATH)\n",
    "    env = UnityEnvironment(file_name=\"environment\", seed=1, side_channels=[],base_port = 5004)\n",
    "    env.step()\n",
    "    # in this project, we are only using one agent, so we will only work on the first `brain` in the environmet\n",
    "    # get the default brain\n",
    "    # brain_name = env.brain_names[0]\n",
    "    brain_name = list(env.behavior_specs.keys())[0]\n",
    "    # brain = env.brains[brain_name]\n",
    "    brain = env.behavior_specs[brain_name]\n",
    "    return env, brain, brain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, brain, brain_name = initialise_env(ENV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Behavior?team=0\n"
     ]
    }
   ],
   "source": [
    "print(brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先让他走几步直到需要交互\n",
    "env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BehaviorSpec(observation_specs=[ObservationSpec(shape=(128, 128, 3), dimension_property=(<DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.NONE: 1>), observation_type=<ObservationType.DEFAULT: 0>, name='CameraSensor'), ObservationSpec(shape=(0,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor'), ObservationSpec(shape=(4,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size4')], action_spec=ActionSpec(continuous_size=0, discrete_branches=(5,)))\n"
     ]
    }
   ],
   "source": [
    "spec = env.behavior_specs['My Behavior?team=0']\n",
    "print(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "#env_info = env.reset(train_mode=True)[brain_name]\n",
    "env_info = env.reset()\n",
    "#action_size = brain.vector_action_space_size\n",
    "#state_size = len(env_info.vector_observations[0])\n",
    "#agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(env_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = list(brain.action_spec)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_steps, terminal_steps = env.get_steps(brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = np.array(decision_steps.obs).resize(84,84)\n",
    "# #state = np.reshape(state,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = len(brain.observation_specs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_size = len(env_info.vector_observations[0])\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = []\n",
    "for act in range(5):\n",
    "    a.append(spec.action_spec.empty_action(1))\n",
    "    a[act].add_discrete(np.int32([[act]]))\n",
    "stop = a[0]\n",
    "forward = a[1]\n",
    "backward = a[2]\n",
    "turn_right = a[3]\n",
    "turn_left = a[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(x):\n",
    "    \"\"\" plots first ncols images in a batch \"\"\"\n",
    "    x = x.squeeze(1).T\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n",
    "    ax.imshow(x, cmap=\"Greys\")\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as transforms\n",
    "\n",
    "def train_dqn(agent, n_episodes=2, max_t=15, eps_start=1.0, eps_end=0.1, eps_decay=0.98):\n",
    "\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "\n",
    "    ======\n",
    "\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "\n",
    "        eps_end (float): minimum value of epsilon\n",
    "\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    scores = []                        # list containing scores from each episode\n",
    "\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    reward = 0\n",
    "    rewards = 0\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "\n",
    "        #state = brain.observation_specs[0]\n",
    "\n",
    "        decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "\n",
    "        state = np.moveaxis(decision_steps.obs[0], -1, 0)\n",
    "\n",
    "        # 看有几个agent\n",
    "\n",
    "        tracked_agent = -1\n",
    "\n",
    "        done = False\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        for t in range(max_t):\n",
    "\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "#             show_images(state)\n",
    "                \n",
    "\n",
    "            #env.set_actions(behavior_name, forward)\n",
    "\n",
    "            action = agent.act(state, eps)\n",
    "\n",
    "#             packed_action = spec.action_spec.empty_action(1)\n",
    "\n",
    "#             packed_action.add_discrete(np.int32([[action]]))\n",
    "\n",
    "#             print(action, brain_name)\n",
    "#             print(action)\n",
    "\n",
    "            env.set_actions(brain_name, a[action])\n",
    "\n",
    "            env.step()\n",
    "\n",
    "           \n",
    "\n",
    "            decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "\n",
    "#             print(\"~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "#             if tracked_agent in decision_steps: # The agent requested a decision\n",
    "\n",
    "#                 episode_rewards += decision_steps[tracked_agent].reward\n",
    "\n",
    "#             if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "\n",
    "#                 episode_rewards += terminal_steps[tracked_agent].reward\n",
    "\n",
    "#                 done = True\n",
    "\n",
    "            next_state =  np.moveaxis(decision_steps.obs[0], -1, 0)   # get the next state\n",
    "    \n",
    "            if len(next_state) == 0:\n",
    "                continue\n",
    "\n",
    "            if tracked_agent in decision_steps:# The agent requested a decision\n",
    "\n",
    "                reward += decision_steps[tracked_agent].reward  # get the reward\n",
    "\n",
    "                agent.step(state, action, reward, next_state, False)\n",
    "\n",
    "            if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "\n",
    "                rewards += terminal_steps[tracked_agent].reward# get the reward\n",
    "\n",
    "                agent.step(state, action, reward, next_state, True)\n",
    "\n",
    "                break\n",
    "\n",
    "            # done = env_info.local_done[0]  \n",
    "\n",
    "            # print(terminal_steps[0])\n",
    "\n",
    "            # _,_, done = terminal_steps[0]\n",
    "\n",
    "            # done = False\n",
    "\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            # if done:\n",
    "\n",
    "            #     break\n",
    "\n",
    "        scores_window.append(score)       # save most recent score\n",
    "\n",
    "        scores.append(score)              # save most recent score\n",
    "#         print(\"~~~~~~~~~~~~~2~~~~~~~~~~~~~~~~~\")\n",
    "        print(scores_window)\n",
    "\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "\n",
    "#         if i_episode % 100 == 0:\n",
    "\n",
    "#             print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "#             print('saved temporary learned weight')\n",
    "\n",
    "#         if np.mean(scores_window)>=13.0:\n",
    "\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "#             print('agent done training')\n",
    "\n",
    "#             break\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "deque([80.0], maxlen=100)\n",
      "Episode 1\tAverage Score: 80.000.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "deque([80.0, 290.0], maxlen=100)\n",
      "Episode 2\tAverage Score: 185.000.9025\n",
      "0.9025\n",
      "0.9025\n",
      "0.9025\n",
      "0.9025\n",
      "0.9025\n",
      "0.9025\n",
      "0.9025\n",
      "0.9025\n",
      "0.9025\n",
      "0.9025\n",
      "0.9025\n",
      "0.9025\n",
      "0.9025\n",
      "deque([80.0, 290.0, 300.0], maxlen=100)\n",
      "Episode 3\tAverage Score: 223.330.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "0.8573749999999999\n",
      "deque([80.0, 290.0, 300.0, 377.0], maxlen=100)\n",
      "Episode 4\tAverage Score: 261.750.8145062499999999\n",
      "0.8145062499999999\n",
      "0.8145062499999999\n",
      "0.8145062499999999\n",
      "0.8145062499999999\n",
      "0.8145062499999999\n",
      "0.8145062499999999\n",
      "0.8145062499999999\n",
      "0.8145062499999999\n",
      "0.8145062499999999\n",
      "0.8145062499999999\n",
      "0.8145062499999999\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0], maxlen=100)\n",
      "Episode 5\tAverage Score: 326.400.7737809374999999\n",
      "0.7737809374999999\n",
      "0.7737809374999999\n",
      "0.7737809374999999\n",
      "0.7737809374999999\n",
      "0.7737809374999999\n",
      "0.7737809374999999\n",
      "0.7737809374999999\n",
      "0.7737809374999999\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0], maxlen=100)\n",
      "Episode 6\tAverage Score: 360.670.7350918906249998\n",
      "0.7350918906249998\n",
      "0.7350918906249998\n",
      "0.7350918906249998\n",
      "0.7350918906249998\n",
      "0.7350918906249998\n",
      "0.7350918906249998\n",
      "0.7350918906249998\n",
      "0.7350918906249998\n",
      "0.7350918906249998\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0], maxlen=100)\n",
      "Episode 7\tAverage Score: 379.860.6983372960937497\n",
      "0.6983372960937497\n",
      "0.6983372960937497\n",
      "0.6983372960937497\n",
      "0.6983372960937497\n",
      "0.6983372960937497\n",
      "0.6983372960937497\n",
      "0.6983372960937497\n",
      "0.6983372960937497\n",
      "0.6983372960937497\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0], maxlen=100)\n",
      "Episode 8\tAverage Score: 404.250.6634204312890623\n",
      "0.6634204312890623\n",
      "0.6634204312890623\n",
      "0.6634204312890623\n",
      "0.6634204312890623\n",
      "0.6634204312890623\n",
      "0.6634204312890623\n",
      "0.6634204312890623\n",
      "0.6634204312890623\n",
      "0.6634204312890623\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0], maxlen=100)\n",
      "Episode 9\tAverage Score: 445.890.6302494097246091\n",
      "0.6302494097246091\n",
      "0.6302494097246091\n",
      "0.6302494097246091\n",
      "0.6302494097246091\n",
      "0.6302494097246091\n",
      "0.6302494097246091\n",
      "0.6302494097246091\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0], maxlen=100)\n",
      "Episode 10\tAverage Score: 483.800.5987369392383786\n",
      "0.5987369392383786\n",
      "0.5987369392383786\n",
      "0.5987369392383786\n",
      "0.5987369392383786\n",
      "0.5987369392383786\n",
      "0.5987369392383786\n",
      "0.5987369392383786\n",
      "0.5987369392383786\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0], maxlen=100)\n",
      "Episode 11\tAverage Score: 523.000.5688000922764596\n",
      "0.5688000922764596\n",
      "0.5688000922764596\n",
      "0.5688000922764596\n",
      "0.5688000922764596\n",
      "0.5688000922764596\n",
      "0.5688000922764596\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0], maxlen=100)\n",
      "Episode 12\tAverage Score: 555.670.5403600876626365\n",
      "0.5403600876626365\n",
      "0.5403600876626365\n",
      "0.5403600876626365\n",
      "0.5403600876626365\n",
      "0.5403600876626365\n",
      "0.5403600876626365\n",
      "0.5403600876626365\n",
      "0.5403600876626365\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0], maxlen=100)\n",
      "Episode 13\tAverage Score: 583.310.5133420832795047\n",
      "0.5133420832795047\n",
      "0.5133420832795047\n",
      "0.5133420832795047\n",
      "0.5133420832795047\n",
      "0.5133420832795047\n",
      "0.5133420832795047\n",
      "0.5133420832795047\n",
      "0.5133420832795047\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0], maxlen=100)\n",
      "Episode 14\tAverage Score: 606.640.48767497911552943\n",
      "0.48767497911552943\n",
      "0.48767497911552943\n",
      "0.48767497911552943\n",
      "0.48767497911552943\n",
      "0.48767497911552943\n",
      "0.48767497911552943\n",
      "0.48767497911552943\n",
      "0.48767497911552943\n",
      "0.48767497911552943\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0], maxlen=100)\n",
      "Episode 15\tAverage Score: 626.200.46329123015975293\n",
      "0.46329123015975293\n",
      "0.46329123015975293\n",
      "0.46329123015975293\n",
      "0.46329123015975293\n",
      "0.46329123015975293\n",
      "0.46329123015975293\n",
      "0.46329123015975293\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0, 887.0], maxlen=100)\n",
      "Episode 16\tAverage Score: 642.500.44012666865176525\n",
      "0.44012666865176525\n",
      "0.44012666865176525\n",
      "0.44012666865176525\n",
      "0.44012666865176525\n",
      "0.44012666865176525\n",
      "0.44012666865176525\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0, 887.0, 844.0], maxlen=100)\n",
      "Episode 17\tAverage Score: 654.350.41812033521917696\n",
      "0.41812033521917696\n",
      "0.41812033521917696\n",
      "0.41812033521917696\n",
      "0.41812033521917696\n",
      "0.41812033521917696\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0, 887.0, 844.0, 840.0], maxlen=100)\n",
      "Episode 18\tAverage Score: 664.670.3972143184582181\n",
      "0.3972143184582181\n",
      "0.3972143184582181\n",
      "0.3972143184582181\n",
      "0.3972143184582181\n",
      "0.3972143184582181\n",
      "0.3972143184582181\n",
      "0.3972143184582181\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0, 887.0, 844.0, 840.0, 990.0], maxlen=100)\n",
      "Episode 19\tAverage Score: 681.790.37735360253530714\n",
      "0.37735360253530714\n",
      "0.37735360253530714\n",
      "0.37735360253530714\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0, 887.0, 844.0, 840.0, 990.0, 979.0], maxlen=100)\n",
      "Episode 20\tAverage Score: 696.650.35848592240854177\n",
      "0.35848592240854177\n",
      "0.35848592240854177\n",
      "0.35848592240854177\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0, 887.0, 844.0, 840.0, 990.0, 979.0, 945.0], maxlen=100)\n",
      "Episode 21\tAverage Score: 708.480.34056162628811465\n",
      "0.34056162628811465\n",
      "0.34056162628811465\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0, 887.0, 844.0, 840.0, 990.0, 979.0, 945.0, 945.0], maxlen=100)\n",
      "Episode 22\tAverage Score: 719.230.3235335449737089\n",
      "0.3235335449737089\n",
      "0.3235335449737089\n",
      "0.3235335449737089\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0, 887.0, 844.0, 840.0, 990.0, 979.0, 945.0, 945.0, 945.0], maxlen=100)\n",
      "Episode 23\tAverage Score: 729.040.30735686772502346\n",
      "0.30735686772502346\n",
      "0.30735686772502346\n",
      "0.30735686772502346\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0, 887.0, 844.0, 840.0, 990.0, 979.0, 945.0, 945.0, 945.0, 917.0], maxlen=100)\n",
      "Episode 24\tAverage Score: 736.880.2919890243387723\n",
      "0.2919890243387723\n",
      "0.2919890243387723\n",
      "0.2919890243387723\n",
      "0.2919890243387723\n",
      "0.2919890243387723\n",
      "0.2919890243387723\n",
      "0.2919890243387723\n",
      "0.2919890243387723\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0, 887.0, 844.0, 840.0, 990.0, 979.0, 945.0, 945.0, 945.0, 917.0, 885.0], maxlen=100)\n",
      "Episode 25\tAverage Score: 742.800.27738957312183365\n",
      "0.27738957312183365\n",
      "deque([80.0, 290.0, 300.0, 377.0, 585.0, 532.0, 495.0, 575.0, 779.0, 825.0, 915.0, 915.0, 915.0, 910.0, 900.0, 887.0, 844.0, 840.0, 990.0, 979.0, 945.0, 945.0, 945.0, 917.0, 885.0, 354.0], maxlen=100)\n",
      "Episode 26\tAverage Score: 727.85"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 3, 0, 128, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6800/2355000723.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#     # load the weights from file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#     agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6800/3116359723.py\u001b[0m in \u001b[0;36mtrain_dqn\u001b[1;34m(agent, n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m#env.set_actions(behavior_name, forward)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m#             packed_action = spec.action_spec.empty_action(1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\DIC_A3-main\\environment\\dqn_agent.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, eps)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0maction_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\DIC_A3-main\\environment\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;34m\"\"\"Build a network that maps state -> action values.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m#         print(\"~~~~~~~~~~~~~~~~~~~~~\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 443\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 3, 0, 128, 128]"
     ]
    }
   ],
   "source": [
    "# if os.path.isfile('./checkpoint.pth'):\n",
    "#     # load the weights from file\n",
    "#     agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "scores = train_dqn(agent, n_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./checkpoint.pth'):\n",
    "    # load the weights from file\n",
    "    agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch a trained agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_banana_agent(agent, env, n_episodes=4, n_steps=300):\n",
    "\n",
    "                                   \n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        \n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        score = 0                                          # initialize the score\n",
    "        \n",
    "        for step in range(n_steps):\n",
    "\n",
    "            action = agent.act(state)                 # select an action\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            score += reward                                # update the score\n",
    "            state = next_state                             # roll over the state to next time step\n",
    "            if done:                                       # exit loop if episode finished\n",
    "                break\n",
    "\n",
    "        print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 3.0\n",
      "Score: 9.0\n",
      "Score: 8.0\n",
      "Score: 20.0\n"
     ]
    }
   ],
   "source": [
    "watch_banana_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
